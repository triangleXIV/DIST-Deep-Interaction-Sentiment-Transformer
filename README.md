# DIST-Deep-Interaction-Sentiment-Transformer

[中文](https://github.com/triangleXIV/DIST-Deep-Interaction-Sentiment-Transformer/blob/main/swin/README.md)||English

This GitHub repository contains the implementation of the paper "Breaking Through Misconceptions: The Leap of Transformer on the Chinese ACMSC Dataset." To highlight the model's multilingual and multi-domain capabilities, I plan to add a private Vietnamese ACMSC dataset later.

When writing this paper, I was under immense pressure to complete the submission for a journal (if rejected again, I would be at significant risk of delaying my graduation, Orz). Fortunately, the editor-in-chief and reviewers were very kind and provided many suggestions and great support for this paper, for which I am deeply grateful.
<div align="center"> <img src="https://github.com/triangleXIV/DIST-Deep-Interaction-Sentiment-Transformer/blob/main/swin/1.jpg" alt="Image text" width="25%"> </div>

In simple terms, I consider this paper an incremental study on the [HIMT model](https://github.com/NUSTM/HIMT). While reading the original HIMT paper, I noticed it used three datasets: Multi-ZOL and Twitter. Interestingly, the HIMT model showed more significant performance improvements on the Chinese Multi-ZOL dataset than on the English Twitter dataset. However, the paper and related code primarily focused on analyzing the Twitter dataset. This observation inspired me to follow a similar approach but focus on the Multi-ZOL dataset, which eventually led to this paper.

First, I located the Multi-ZOL dataset (without punctuation) and started training. Regardless of whether I trained for eight or ten epochs, the metrics remained almost unchanged at around 64-66% (the original paper reported slightly lower results, likely due to bad luck during their initial runs). Later, after running the model several more times, I confirmed that the performance without punctuation stayed at 64-66%.

At this point, I realized that the Multi-ZOL .txt file lacked punctuation marks, which are encoded in tokenizers. So, I retrieved the original .json dataset and added punctuation. Following the HIMT model's approach, I trained the model for eight epochs, achieving results very close to HIMT, around 66%. However, after running additional trials, the metrics suddenly jumped to over 70%. I then increased the training epochs from 8 to 10, which stabilized the results at over 70%. This means that adding punctuation enabled the model to continue improving beyond the eighth epoch—a fascinating discovery.

Next came the ablation experiments. I modified the HIMT model, specifically removing the ARM module (which calculates pre- and post-cross-attention loss). In my opinion, the ARM module was not particularly useful. It might only benefit the original BERT or scenarios lacking semantic information by leveraging this additional loss to achieve a lucky result. While it may produce surprising effects in some cases, it was ineffective for my model, so I removed it.

Finally, this model is primarily designed for scenarios where a single sentence corresponds to multiple images. Consequently, its performance on the Twitter dataset, which involves one-to-one mappings, is relatively weaker. For the Twitter dataset, the optimal solution is likely a generative model. This would involve inputting text and images and having the model generate a response in the format: "[Entity] is [MASK] of," where MASK represents the sentiment word generated by the model. If your research focuses solely on the Twitter dataset, I recommend not using my model.

To fully run this code, you need to perform the following steps:

## 1.Download the Dataset
In this step, you can choose to use the dataset provided by MIMN or the processed dataset. If you choose the MIMN dataset, proceed to step 2; otherwise, go directly to step 3.

MIMN: https://github.com/xunan0812/MIMN

Or choose the processed files (datasets folder)：[onedrive](https://1drv.ms/u/s!Akl56EV1csnmoxnRe49FfF3aBpfb?e=jhp7BC)||[123pan](https://www.123pan.com/s/f3giVv-F1l3H.html)

## 2.Complete Missing Information
Find the multi_jsonData folder from the ZOL dataset and place it in the project's root directory. Then execute construct.py and shuffle.py to rebuild the dataset for constructing missing semantic information.

## 3.Download Pre-trained Models
The pre-trained models used in this study are from Hugging Face and GitHub:

DeBERTa-ZH: https://huggingface.co/IDEA-CCNL/Erlangshen-DeBERTa-v2-97M-Chinese

RoBERTa-ZH: https://huggingface.co/IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment

Swin-Transformer: https://github.com/microsoft/Swin-Transformer

Depending on your needs, choose either DeBERTa([onedrive](https://1drv.ms/u/s!Akl56EV1csnmoxZ9oL-ZCEXMgxMA?e=YEA7Bo))||[123pan](https://www.123pan.com/s/f3giVv-B1l3H.html))or RoBERTa([onedrive](https://1drv.ms/u/s!Akl56EV1csnmoxdO44_IGvg4Eg2F?e=O9K6ZY))||[123pan](https://www.123pan.com/s/f3giVv-J1l3H.html))model((pretrains folder)，The DeBERTa model has higher accuracy compared to RoBERTa (by 1-2%), but it requires double the GPU memory and training time.

## 4.Download Persistently Saved Data (Optional)
Since data preprocessing takes a lot of time, this step directly provides the preprocessed data. Simply extract it into the datasets folder. If you do not download the persistently saved data, ensure your computer has 16GB of GPU memory and wait about an hour to complete the data preprocessing.

DeBERTa: [onedrive](https://1drv.ms/u/s!Akl56EV1csnmoxQQbvZzdAfy7GDP?e=eUWK3v)||[123pan](https://www.123pan.com/s/f3giVv-I1l3H.html)

RoBERTa: [onedrive](https://1drv.ms/u/s!Akl56EV1csnmoxMqYytx4Z9BKdGm?e=n4Zeeu)||[123pan](https://www.123pan.com/s/f3giVv-w1l3H.html)

## 5.Training
Ensure the project structure is as shown below, then execute main.py:
```
├─datasets
│  ├─dev
│  │  └─swin
│  ├─img
│  ├─test
│  │  └─swin
│  └─train
│      └─swin
├─pretrains
└─swin
```

## (Optional) Training with Twitter Dataset
If you want to train with the Twitter15/17 dataset, download the Twitter dataset and rename the image folder to img, then place train, dev, and test.txt files into the datasets folder. Overwrite the original root directory files with the two .py files from the replace folder in this project.
Twitter15/17:[onedrive](https://1drv.ms/u/s!Akl56EV1csnmoxghAlL0TnfUDZWd?e=cFmm5O)||[123an](https://www.123pan.com/s/f3giVv-w1l3H.html)
